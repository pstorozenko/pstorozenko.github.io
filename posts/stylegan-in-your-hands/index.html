<!doctype html><html lang=en><head><title>StyleGANv2 in Your Hands ðŸ’‡ðŸ‘» ::
Pasza's blog â€” a simple blog about complex things</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This is the second post from micro-series preceding the SFI conference. In the first post I showed how easy it is to do the style transfer at home. Now, I would like to present you how to play with StyleGANv2. StyleGANv2 is the second version of StyleGAN, they are very similar in core principals. We will be working with StyleGANv2 but I will refer to it as SG. There is a lot that can be said/explained about SG."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/posts/stylegan-in-your-hands/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.css integrity=sha384-ysFyB7Is//Q1JNgERb0bLJokXKM8eWJsjEutGvthoHtBilHWgbdmbYkQZdwCIGIq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.js integrity=sha384-UWjC+k927Mtx6WQF5SzKTXLLrOYmzs69HvkUjiKvUwSOljzc+C6PrGquNpOvJBBo crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PR8Z9SJC7F"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PR8Z9SJC7F",{anonymize_ip:!1})}</script><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/img/favicon.png><link href=/assets/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/assets/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="StyleGANv2 in Your Hands ðŸ’‡ðŸ‘»"><meta name=twitter:description content="This is the second post from micro-series preceding the SFI conference. In the first post I showed how easy it is to do the style transfer at home. Now, I would like to present you how to play with StyleGANv2. StyleGANv2 is the second version of StyleGAN, they are very similar in core principals. We will be working with StyleGANv2 but I will refer to it as SG. There is a lot that can be said/explained about SG."><meta property="og:title" content="StyleGANv2 in Your Hands ðŸ’‡ðŸ‘»"><meta property="og:description" content="This is the second post from micro-series preceding the SFI conference. In the first post I showed how easy it is to do the style transfer at home. Now, I would like to present you how to play with StyleGANv2. StyleGANv2 is the second version of StyleGAN, they are very similar in core principals. We will be working with StyleGANv2 but I will refer to it as SG. There is a lot that can be said/explained about SG."><meta property="og:type" content="article"><meta property="og:url" content="/posts/stylegan-in-your-hands/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-14T22:30:09+01:00"><meta property="article:modified_time" content="2022-03-14T22:30:09+01:00"><meta property="og:site_name" content="Pasza's blog"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>pasza's blog</span>
<span class=logo__cursor></span></a>
<span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About me</a></li><li><a href=/useful-materials>Useful materials</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About me</a></li><li><a href=/useful-materials>Useful materials</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h1 class=post-title>StyleGANv2 in Your Hands ðŸ’‡ðŸ‘»</h1><div class=post-meta><span class=post-date>2022-03-14</span></div><span class=post-tags><a href=/tags/python/>#python</a>&nbsp;
<a href=/tags/machine-learning/>#machine-learning</a>&nbsp;
<a href=/tags/conference/>#conference</a>&nbsp;
<a href=/tags/sfi-2022/>#SFI 2022</a>&nbsp;</span><div class=post-content><p>This is the second post from micro-series preceding the SFI conference.
In <a href=/posts/playing-with-style-transfer>the first post</a> I showed how easy it is to do the style transfer at home.
Now, I would like to present you how to play with <a href=https://github.com/NVlabs/stylegan2>StyleGANv2</a>.
StyleGANv2 is <a href=https://indipest.files.wordpress.com/2021/03/bw6d5zz.gif>the second version of StyleGAN</a>, they are very similar in core principals.
We will be working with StyleGANv2 but I will refer to it as SG.
There is a lot that can be said/explained about SG.
Here I would like to focus on showing how to make your own experiments with SG instead of getting into SG&rsquo;s details.</p><p>The post is divided in two parts.
In the first, I present some results and ideas behind them.
<a href=/posts/stylegan-in-your-hands#stylegan-and-e4e-at-home>In the second</a>, I show how to obtain similar results and what can you do <strong>on your own computer</strong> (with an access to the internet).</p><h2 id=what-do-i-need-to-know-about-stylegan-before-the-experiments>What do I need to know about StyleGAN before the experiments?</h2><h3 id=simple-face-generations>Simple face generations</h3><p>StyleGAN is a computer vision model that is able to create a high-resolution portraits (1024x1024px) from a random vector $z$ of length 512 (sic!).</p><p><img src=white_lady.jpg alt></p><p>This photorealistic image has been generated from random vector $z$.
This <em>person</em> <strong>DOES NOT EXIST</strong>!
Isn&rsquo;t that crazy?
It&rsquo;s just the beginning.</p><p>Actually SG first transforms vector $z_1\in \mathbb{R}^{512}$ to set of style vectors $w_1\in\mathbb{R}^{512 \times 18}$ and generates face from them.
This is important for our later experiments.</p><p>As for now let us sample different $z_2$ and generate face from this vector.</p><p><img src=smile_black.jpg alt></p><h3 id=style-vectors-mixing>Style vectors mixing</h3><p>Researchers discovered that the first style vectors of $w$ are responsible for general shape of face, gender, while some later are responsible for style, i.e., color of skin, hair.</p><p>Since almost any vector $w$ (created from random vector $z$) will give us <em>some</em> face, we can also interpolate between different $w$!
This will happen if we take vectors 1 to 10 from $w_1$, and vectors 11 to 18 from $w_2$:</p><p><img src=black_lady2.jpg alt></p><p>Crazy, huh?
It goes further, we can take less layers from $w_1$, for example take layers 1 to 7 from $w_1$ and 8 to 18 from $w_2$.
This introduces more features from $w_2$ into result:</p><p><img src=black_guy2.jpg alt></p><p>We can also generate faces from $w_i$ interpolated between $w_1$ and $w_2$.
For example if we would like to have $n=30$ faces in-between we can create them from the following $w_i$</p><p>$$
w_k = \left(1 - \frac{k}{n}\right) w_1 + \frac{k}{n} w_2, \quad k = 1..n
$$</p><p>Animation from generated faces from $w_i$</p><p><img src=man_woman.webp alt></p><h3 id=excuse-me-where-is-the-age-direction>&ldquo;Excuse me, where is the age direction?&rdquo;</h3><p>There exist <a href=https://github.com/genforce/interfacegan>particular <strong>linear</strong> directions</a> in $w$ space, that are responsible for aging or turning head!
This means that once we discover direction $a_{pose}$ we can rotate the face to the left by generating face from $w_1 - 5 a_{pose}$, we can also rotate face to the right by generating from $w_1 + 5 a_{pose}$.
We can also generate the image for some intermediate poses and create the gif:</p><p><img src=rotate_woman.webp alt></p><h3 id=real-face-editing>Real face editing</h3><p>So far we have operated on faces generated from random vectors.
This might be a great fun, but we might want to <em>edit</em> some photo using StyleGAN.
Surprisingly enough it is <em>somehow</em> possible with <a href=https://github.com/omertov/encoder4editing>encoder4editing</a>!
Using this technique along with aging and pose directions we can create interesting mashups.
Following a common pattern to use <a href=https://github.com/ageitgey/face_recognition>the US presidents</a> for benchmarking <a href=https://en.wikipedia.org/wiki/Computer_vision>CV</a> algorithms, I present you the mashup between aging presidents Andrzej Duda and Volodymyr Zelenskyy.</p><p><img src=presidents.webp alt></p><h2 id=stylegan-and-e4e-at-home>StyleGAN and e4e at home</h2><p>Similarly to the previous post, encoder4editing model authors provided <a href=https://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb>a colab notebook for experimenting</a>!</p><p><a href=http://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><p>Here it will not be as easy and straightforward as earlier because the model is not included in any kind of model zoo.
You have to follow the notebook step by step, cells might require some time to run.
On one point, you will have to grant the Google Cloud SDK access to your Google Drive.
It will be necessary to download the pretrained e4e model and load it into your colab environment (model weights over 1GB), don&rsquo;t be scared (the code will do it automatically for you).</p><p>Some explanations regarding experiments in the notebook:</p><ul><li><code>ffhq_encode</code> - our focus, StyleGAN trained on <a href=https://github.com/NVlabs/ffhq-dataset>FFHQ - dataset of high quality images</a></li><li><code>cars_encode</code> - StyleGAN on cars</li><li><code>horse_encode</code> - StyleGAN on horse</li><li><code>church_encode</code> - StyleGAN on church</li></ul><h3 id=working-with-real-images-approximations>Working with real images approximations</h3><p>To provide your own photo for e4e to use, you can upload your photo to colab (I explained it <a href=/posts/playing-with-style-transfer#adjusting-for-images-from-you-local-disk>in the previous post</a>) and change the first line in cell:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>image_path <span style=color:#f92672>=</span> EXPERIMENT_DATA_ARGS[experiment_type][<span style=color:#e6db74>&#34;image_path&#34;</span>]
</span></span><span style=display:flex><span>original_image <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(image_path)
</span></span><span style=display:flex><span>original_image <span style=color:#f92672>=</span> original_image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#34;RGB&#34;</span>)
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>image_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;path_to_image.jpg&#34;</span>
</span></span></code></pre></div><p>In section:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>    tic <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    images, latents <span style=color:#f92672>=</span> run_on_batch(transformed_image<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>), net)
</span></span><span style=display:flex><span>    result_image, latent <span style=color:#f92672>=</span> images[<span style=color:#ae81ff>0</span>], latents[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    toc <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;Inference took </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74> seconds.&#39;</span><span style=color:#f92672>.</span>format(toc <span style=color:#f92672>-</span> tic))
</span></span></code></pre></div><p>you get <code>latents</code>, the key part of what you need.
This is what we called $w$ and is later used to generate the face.</p><p>If you have <code>latents</code>, you can create image from them with</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>  img_t, _ <span style=color:#f92672>=</span> net<span style=color:#f92672>.</span>decoder(latents<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>), input_is_latent<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, randomize_noise<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>  img_t <span style=color:#f92672>=</span> img_t<span style=color:#f92672>.</span>squeeze(<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>The resulting <code>img_t</code> is <code>[3, 1024, 1024]</code> tensor that can be converted to image with
<code>img = tensor2im(img)</code>.
It is convenient to resize images to something smaller with <code>img.resize((256, 256))</code> as working with 1024x1024 images is rather slow in colab from my experience.</p><h3 id=creating-random-images>Creating random images</h3><p>In this implementation of SG you have to create all 18 $w$/<code>latents</code> from separate $z$.
For example like that:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>latents <span style=color:#f92672>=</span> net<span style=color:#f92672>.</span>decoder<span style=color:#f92672>.</span>get_latent(torch<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>18</span>, <span style=color:#ae81ff>512</span>), device<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cuda:0&#34;</span>))<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>Feed it into the earlier code to get your first generated face!</p><p>If you have latents <code>l1</code> and <code>l2</code> you want to create face from first 8 layers of <code>l1</code> and last 10 of <code>l2</code> you can do:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>l12 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat((l1[:, :<span style=color:#ae81ff>8</span>, :], l2[:, <span style=color:#ae81ff>8</span>:, :]), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>In my case I got this scarry face this time.
Please, do not blame me for your nightmares later. ðŸ‘»</p><p><img src=strange_mix.jpg alt></p><h2 id=why-this-all-is-possible>Why this all is possible</h2><p>While this all is magnificent we can only speculate why random vectors are giving us reasonable faces and why those age/rotation directions exist.
People say it is because the latent space of size 512 is very dense in terms of contained information.
It has to be able to carry information on about 1024x1024 pixel of people of all ethnicities, ages.
We can choose almost any point in $R^{512}$ and it will produce plausible results.
It is also important to remember that the network saw all those ages and ethnicities in the training data.</p><p>This also explains the existence of directions responsible for aging and face rotation.
Since network saw a lot of faces rotated by different angles, this information located in latent space.
The same goes for aging.
On the other hand there is no up-down head direction in SG as there were not that much examples of people tilting forward and backward their heads in the training data.
Network can generate brilliant results, but only of the same kind as it saw.</p><h2 id=summary>Summary</h2><p>In this post I presented what can be achieved with StyleGAN and how you can play with it using free google colab.
I hope you liked that and your experiments were fruitful ;)</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button next"><a href=/posts/solar-panels-4/><span class=button__text>Streamlit Tutorial: How to Deploy Streamlit Apps on RStudio Connect ðŸ’°</span>
<span class=button__icon>â†’</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user">Piotr Pasza StoroÅ¼enko</div></div></footer><script src=/assets/main.js></script>
<script src=/assets/prism.js></script></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-PR8Z9SJC7F"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PR8Z9SJC7F",{anonymize_ip:!1})}</script></body></html>